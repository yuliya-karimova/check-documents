{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number                                        Name  \\\n",
      "0  30365  Start the Charging Process via Soft Switch   \n",
      "\n",
      "                                            Text_HMI Text_SSTS  \n",
      "0  [I-30365]  Start the Charging Process via Soft...       NaN  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "# openai предоставляет только библиотеку для формирования запросов к языковым моделям\n",
    "# ChatGPT не используется\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "def load_documents_from_folder(path, prefix):\n",
    "    # Собираем список файлов в папке\n",
    "    files = [f for f in os.listdir(path) if f.endswith('.docx') and f.startswith(prefix)]\n",
    "    data = []\n",
    "    for file in files:\n",
    "        # Извлекаем номер из названия файла\n",
    "        match = re.search(rf'{prefix}(\\d+)', file)\n",
    "        if match:\n",
    "            file_number = match.group(1)\n",
    "        else:\n",
    "            continue  # Пропускаем файлы без номера\n",
    "\n",
    "        # Полный путь к файлу\n",
    "        filepath = os.path.join(path, file)\n",
    "        doc = Document(filepath)\n",
    "        paragraphs = [p.text for p in doc.paragraphs]  # Извлекаем текст из всех абзацев\n",
    "        if paragraphs:\n",
    "            first_line = paragraphs[0]\n",
    "            name_match = re.search(r'\\](.*)', first_line)\n",
    "            if name_match:\n",
    "                document_name = name_match.group(1).strip()  # Извлекаем название документа\n",
    "            else:\n",
    "                document_name = \"No Name Found\"  # Заглушка, если название не найдено\n",
    "        else:\n",
    "            document_name = \"Empty Document\"  # Заглушка для пустых документов\n",
    "\n",
    "        full_text = '\\n'.join(paragraphs)  # Объединяем текст в одну строку\n",
    "        data.append({'Number': file_number, 'Name': document_name, 'Text': full_text})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Пути к папкам\n",
    "path_hmi = './HMI'\n",
    "path_stss = './SSTS'\n",
    "\n",
    "# Загружаем документы из обеих папок\n",
    "df_hmi = load_documents_from_folder(path_hmi, 'UC-')\n",
    "df_stss = load_documents_from_folder(path_stss, 'SSTS-')\n",
    "\n",
    "# Используем только первый документ из каждого датафрейма для тестирования\n",
    "df_hmi = df_hmi.head(1)\n",
    "df_stss = df_stss.head(1)\n",
    "\n",
    "# Объединяем датафреймы по номеру документа\n",
    "df = pd.merge(df_hmi, df_stss, on='Number', how='left', suffixes=('_HMI', '_SSTS'))\n",
    "\n",
    "# Дропаем колонку Name_SSTS\n",
    "df = df.drop(columns=['Name_SSTS'])\n",
    "\n",
    "# Переименовываем колонку Name_HMI в Name\n",
    "df = df.rename(columns={'Name_HMI': 'Name'})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "    \n",
    "# Payload example\n",
    "class Payload(BaseModel):\n",
    "    model: Optional[str] = None\n",
    "    messages: List[Message]\n",
    "\n",
    "# Функция `sent_to_ai` предназначена для отправки запроса в ЛОКАЛЬНО развернутую llm модель\n",
    "# Данные никуда не отправляются и остаются в безопасности на вашем устройстве\n",
    "# Она принимает данные в формате словаря (payload), которые содержат параметры для выполнения запроса к модели\n",
    "# В результате выполнения запроса функция получает ответ от модели и возвращает его текстовое содержание\n",
    "\n",
    "# DEFAULT_MODEL = 'llama-3.2-3b-instruct'\n",
    "DEFAULT_MODEL = 'meta-llama-3.1-8b-instruct'\n",
    "    \n",
    "def sent_to_ai(payload: Payload):\n",
    "    if not payload.model:\n",
    "        payload.model = DEFAULT_MODEL\n",
    "        \n",
    "    payload = payload.dict()\n",
    "    \n",
    "    try:\n",
    "        client = OpenAI(base_url=\"http://127.0.0.1:1234/v1/\", api_key=\"lm-studio\")\n",
    "        response = client.chat.completions.create(**payload)\n",
    "        print('response', response)\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        # Обработка любой ошибки\n",
    "        print(f\"Произошла ошибка: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response ChatCompletion(id='chatcmpl-pvkhbeichm9zjw606stf4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='**HMI Summary: Start Charging Process via Soft Switch**\\n\\n1. **Preconditions**\\n\\t* Vehicle in Parking mode\\n\\t* Charging cable plugged in\\n2. **User Interaction**\\n\\t* Navigate to charging interface on out_2 (SWP) or out_5 (Mobile App)\\n\\t* Press \"Start Charging\" soft switch button\\n3. **System Response**\\n\\t* Check if vehicle is in Parking mode and charging cable is plugged in\\n\\t* Send command to start charging process\\n4. **Notifications**\\n\\t* Display notification on out_2 (SWP) or out_5 (Mobile App) that charging session has been started successfully\\n5. **Charging Process Status**\\n\\t* Charging process in progress', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1731224599, model='meta-llama-3.1-8b-instruct', object='chat.completion', service_tier=None, system_fingerprint='meta-llama-3.1-8b-instruct', usage=CompletionUsage(completion_tokens=149, prompt_tokens=360, total_tokens=509, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "def create_short_hmi_description(text_hmi):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following detailed description of an HMI (Human-Machine Interface) and summarize it in a concise and clear format. Focus on the main functions, user interactions, and essential controls or indicators. The summary should be brief, straightforward, and formatted in a way that makes it easy for further verification or testing.\n",
    "\n",
    "    Original HMI Description: \"{text_hmi}\"\n",
    "\n",
    "    Please provide a short summary focusing on the key functionalities and interactions in the HMI. Avoid any additional explanations, and keep the language simple and direct.\n",
    "    \"\"\"\n",
    "    messages = [Message(role=\"user\", content=prompt)]\n",
    "    payload = Payload(messages=messages)\n",
    "    response = sent_to_ai(payload)\n",
    "    return response.strip()  # Возвращаем краткое описание без лишних символов\n",
    "\n",
    "# Применение функции для создания новой колонки 'Short HMI'\n",
    "df['Short HMI'] = df['Text_HMI'].apply(create_short_hmi_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаблон промпта для фильтрации нерелевантного контента\n",
    "keep_only_relevant_content_prompt_template = \"\"\"\n",
    "You are provided with a query and a set of retrieved documents. Your task is to filter out all non-relevant information that does not provide important details regarding the query. \n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Retrieved Documents: {retrieved_documents}\n",
    "\n",
    "Your goal is to keep only the information that is directly relevant to the query, removing any extraneous details. You may remove parts of sentences or entire sentences that are not relevant to the query.\n",
    "\n",
    "DO NOT ADD ANY NEW INFORMATION THAT IS NOT IN THE RETRIEVED DOCUMENTS.\n",
    "\n",
    "Output only the filtered, relevant content.\n",
    "\"\"\"\n",
    "\n",
    "# Модель для вывода релевантного контента\n",
    "class KeepRelevantContent(BaseModel):\n",
    "    relevant_content: str = Field(description=\"The relevant content from the retrieved documents that is relevant to the query.\")\n",
    "\n",
    "# Подготовка промпта для запроса\n",
    "def create_relevant_content_prompt(short_hmi, text_stss):\n",
    "    return keep_only_relevant_content_prompt_template.format(\n",
    "        query=short_hmi,\n",
    "        retrieved_documents=text_stss\n",
    "    )\n",
    "\n",
    "# Функция для фильтрации контента\n",
    "def keep_only_relevant_content(row):\n",
    "    \"\"\"\n",
    "    Filters Text_SSTS to keep only the content relevant to Short HMI.\n",
    "\n",
    "    Args:\n",
    "        row: A row of the DataFrame containing 'Short HMI' and 'Text_SSTS'.\n",
    "\n",
    "    Returns:\n",
    "        str: The filtered relevant content from 'Text_SSTS' based on 'Short HMI'.\n",
    "    \"\"\"\n",
    "    # Проверяем, есть ли данные в Text_STSS\n",
    "    if pd.isna(row['Text_SSTS']):\n",
    "        return \"\"\n",
    "\n",
    "    # Формируем промпт для фильтрации контента\n",
    "    prompt = create_relevant_content_prompt(row['Short HMI'], row['Text_SSTS'])\n",
    "    messages = [Message(role=\"user\", content=prompt)]\n",
    "    payload = Payload(messages=messages)\n",
    "    response = sent_to_ai(payload)\n",
    "\n",
    "    # Возвращаем отфильтрованный релевантный контент\n",
    "    return response.strip()\n",
    "\n",
    "# Применение функции к DataFrame для обновления колонки 'Text_STSS'\n",
    "df['Filtered Text_SSTS'] = df.apply(keep_only_relevant_content, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response ChatCompletion(id='chatcmpl-dt00swaybgdyxt7rczhxm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Since there are no specific requirements provided in \\'Text_SSTS\\', we cannot perform a direct comparison between the Short HMI and the requirements. However, I can provide an example of how Chain-of-Thought Reasoning could be applied if specific requirements were given.\\n\\nAssuming that specific Text_SSTS requirements are available for this context, here\\'s how we would approach it:\\n\\n1.  **The Short HMI states that the charging process can be started via a soft switch button in two ways:**\\n    *   The user interacts with either the SWP (out_2) or Mobile App (out_5).\\n    *   The user presses the \"Start Charging\" soft switch button.\\n\\n2.  **The requirements (Text_SSTS) specify that the HMI must allow users to initiate the charging process in a specific way.**\\n\\n3.  Since we don\\'t have the actual Text_SSTS requirements, let\\'s assume there is one requirement: \"The vehicle\\'s HMI should support initiating the charging process via both mobile app and SWP (SWP means some sort of interface other than Mobile App).\"\\n\\n4.  **Comparing the Short HMI to the requirement:** \\n    *   The Short HMI states that the user can initiate charging through either the SWP or Mobile App.\\n    *   This matches our hypothetical Text_SSTS requirement, as both methods are supported.\\n\\n5.  Therefore, based on this example assumption of a Text_SSTS requirement, the \\'Short HMI\\' meets the requirements outlined in \\'Text_SSTS\\'.\\n\\nHowever, since there is no actual requirement provided in the given context, the question cannot be definitively answered with the available information.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1731224601, model='meta-llama-3.1-8b-instruct', object='chat.completion', service_tier=None, system_fingerprint='meta-llama-3.1-8b-instruct', usage=CompletionUsage(completion_tokens=340, prompt_tokens=593, total_tokens=933, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "def answer_question_from_context(row):\n",
    "    \"\"\"\n",
    "    Answers a question about whether 'Short HMI' meets the requirements in 'Text_SSTS' using a chain-of-thought reasoning approach.\n",
    "\n",
    "    Args:\n",
    "        row: A row of the DataFrame containing 'Short HMI' and 'Text_SSTS'.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer based on the reasoning chain.\n",
    "    \"\"\"\n",
    "    # Формируем контекст и вопрос для цепочки рассуждений\n",
    "    context = f\"\"\"\n",
    "    Short HMI: {row['Short HMI']}\n",
    "    Requirements (Text_SSTS): {row['Text_SSTS']}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    question = \"Does the 'Short HMI' meet the requirements outlined in 'Text_SSTS'?\"\n",
    "\n",
    "    # Промпт с примерами цепочки рассуждений\n",
    "    prompt = f\"\"\"\n",
    "    Examples of Chain-of-Thought Reasoning\n",
    "\n",
    "    Example 1\n",
    "    Context:\n",
    "    Short HMI: \"The vehicle's dashboard displays the current speed in kilometers per hour.\"\n",
    "    Requirements (Text_SSTS): \"The vehicle must display the speed in both kilometers per hour and miles per hour.\"\n",
    "    Question: Does the 'Short HMI' meet the requirements outlined in 'Text_SSTS'?\n",
    "    Reasoning Chain:\n",
    "    The Short HMI states that the dashboard displays the current speed in kilometers per hour.\n",
    "    The requirement specifies that the speed must be displayed in both kilometers per hour and miles per hour.\n",
    "    Since the Short HMI only mentions kilometers per hour, it does not include miles per hour.\n",
    "    Therefore, the 'Short HMI' does not fully meet the requirements outlined in 'Text_SSTS'.\n",
    "\n",
    "    Example 2\n",
    "    Context:\n",
    "    Short HMI: \"The vehicle is equipped with a touchscreen interface that allows users to control the air conditioning system, including temperature settings, fan speed, and airflow direction.\"\n",
    "    Requirements (Text_SSTS): \"The human-machine interface must provide controls for the air conditioning system, including temperature adjustment, fan speed control, and airflow direction selection.\"\n",
    "    Question: Does the 'Short HMI' meet the requirements outlined in 'Text_SSTS'?\n",
    "    Reasoning Chain:\n",
    "    The Short HMI describes a touchscreen interface that allows users to control temperature settings, fan speed, and airflow direction.\n",
    "    The requirements specify that the HMI must provide controls for temperature adjustment, fan speed control, and airflow direction selection.\n",
    "    The Short HMI includes all the controls specified in the requirements.\n",
    "    Therefore, the 'Short HMI' meets the requirements outlined in 'Text_SSTS'.\n",
    "\n",
    "    Now, use the following context and question to provide an answer with step-by-step reasoning:\n",
    "    Context:\n",
    "    {context}\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Подготовка сообщений и отправка запроса к GPT\n",
    "    messages = [Message(role=\"user\", content=prompt)]\n",
    "    payload = Payload(messages=messages)\n",
    "    response = sent_to_ai(payload)\n",
    "    # Возвращаем только ответ\n",
    "    return response.strip()\n",
    "\n",
    "# Применение функции к DataFrame для создания колонки с ответами\n",
    "df['Requirement Compliance Answer'] = df.apply(answer_question_from_context, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response ChatCompletion(id='chatcmpl-mh519lkkwhhjvairtzfl6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"**Key Compliance Issues:**\\n\\n1.  Lack of specific Text_SSTS requirements for comparison.\\n2.  Inability to perform direct comparison between Short HMI and requirements due to missing details.\\n3.  Hypothetical assumption made based on assumed requirement.\\n\\n**Essential Non-Compliance Details:**\\n\\n*   No actual Text_SSTS requirements are provided.\\n*   Short HMI states multiple methods for initiating charging, but there's no basis to confirm if these meet specific requirements.\\n*   The analysis relies on an unverified assumption of a hypothetical Text_SSTS requirement.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1731224605, model='meta-llama-3.1-8b-instruct', object='chat.completion', service_tier=None, system_fingerprint='meta-llama-3.1-8b-instruct', usage=CompletionUsage(completion_tokens=116, prompt_tokens=422, total_tokens=538, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "def summarize_discrepancies(description):\n",
    "    prompt = f\"\"\"\n",
    "    Given a detailed description of compliance discrepancies, summarize the key issues succinctly. Focus only on the main points of non-compliance and essential details that highlight what is wrong or missing. Please provide a concise summary.\n",
    "\n",
    "    Full Description: \"{description}\"\n",
    "    \"\"\"\n",
    "    messages = [Message(role=\"user\", content=prompt)]\n",
    "    payload = Payload(messages=messages)  # Используйте соответствующую модель GPT\n",
    "    response = sent_to_ai(payload)\n",
    "    return response.strip()\n",
    "\n",
    "# Применение функции к DataFrame для создания новой колонки 'Differences'\n",
    "df['Differences'] = df['Requirement Compliance Answer'].apply(summarize_discrepancies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response ChatCompletion(id='chatcmpl-jxtcjckarthj6c49qgwo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='**Charging Process via Soft Switch Summary**\\n\\nThe charging process via soft switch allows users to initiate the charging process remotely through the Mobile App or on-board SWP interface (out_2). To start the process:\\n\\n1.  The vehicle must be in Parking mode and have a plugged-in charging cable.\\n2.  Users navigate to the charging interface, press the \"Start Charging\" soft switch button.\\n3.  The system checks preconditions and sends a command to start the charging process.\\n4.  A notification is displayed upon successful session initiation.\\n\\n**Significant Discrepancies:**\\n\\n*   **Lack of Clear Requirements:** The absence of specific Text_SSTS requirements hinders direct comparison between Short HMI and expectations.\\n*   **Uncertainty about Compliance:** Without concrete requirements, it\\'s challenging to confirm if the described methods meet specified standards.\\n\\nBy acknowledging these discrepancies, users can still understand the core functionality while being aware that further validation is required for complete compliance.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1731224606, model='meta-llama-3.1-8b-instruct', object='chat.completion', service_tier=None, system_fingerprint='meta-llama-3.1-8b-instruct', usage=CompletionUsage(completion_tokens=197, prompt_tokens=509, total_tokens=706, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "def create_description(row):\n",
    "    prompt = f\"\"\"\n",
    "    Given the detailed descriptions from three sources - HMI interface operations, system standards and specifications (STSS), and key discrepancies - synthesize a clear and concise summary that captures the essence of the functionality or process described. Focus on integrating information to provide a comprehensive yet succinct description that is useful for understanding the core functionalities and any critical issues.\n",
    "\n",
    "    HMI Description: \"{row['Text_HMI']}\"\n",
    "    STSS Description: \"{row['Text_SSTS']}\"\n",
    "    Key Discrepancies: \"{row['Differences']}\"\n",
    "\n",
    "    Please produce a summary that combines these details into a streamlined description of the main features and functions, highlighting any significant discrepancies where applicable. The summary should be concise, informative, and only include the most relevant information for clarity and utility.\n",
    "    \"\"\"\n",
    "    messages = [Message(role=\"user\", content=prompt)]\n",
    "    payload = Payload(messages=messages)\n",
    "    response = sent_to_ai(payload)\n",
    "    return response.strip()\n",
    "\n",
    "# Применение функции к DataFrame\n",
    "df['Description'] = df.apply(create_description, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель данных для проверки фактической обоснованности ответа\n",
    "class IsGroundedOnFacts(BaseModel):\n",
    "    grounded_on_facts: str = Field(description=\"Answer 'True' if the answer is based on the provided context, 'False' otherwise.\")\n",
    "\n",
    "# Шаблон промпта для проверки фактической обоснованности ответа\n",
    "is_grounded_on_facts_prompt_template = \"\"\"\n",
    "You are provided with a context and an answer. Your task is to determine if the answer is fully grounded in the information provided within the context. \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Please respond with \"True\" if the answer is completely based on the given context, without any unsupported assumptions or additional information, or \"False\" if the answer includes any hallucinations or unsupported claims.\n",
    "\"\"\"\n",
    "\n",
    "# Функция для создания промпта\n",
    "def create_is_grounded_prompt(context, answer):\n",
    "    return is_grounded_on_facts_prompt_template.format(\n",
    "        context=context,\n",
    "        answer=answer\n",
    "    )\n",
    "\n",
    "# Основная функция для проверки обоснованности ответа\n",
    "def is_answer_grounded_on_context(state):\n",
    "    \"\"\"\n",
    "    Determines if the answer to the question is grounded in the facts.\n",
    "    \n",
    "    Args:\n",
    "        state: A dictionary containing the context and answer.\n",
    "    \n",
    "    Returns:\n",
    "        str: \"grounded on context\" if the answer is based on the context, \"hallucination\" otherwise.\n",
    "    \"\"\"\n",
    "    print(\"Checking if the answer is grounded in the facts...\")\n",
    "    context = state[\"context\"]\n",
    "    answer = state[\"answer\"]\n",
    "\n",
    "    # Создание промпта для проверки\n",
    "    prompt = create_is_grounded_prompt(context, answer)\n",
    "    messages = [Message(role=\"user\", content=prompt)]\n",
    "    payload = Payload(messages=messages)\n",
    "    response = sent_to_ai(payload)\n",
    "\n",
    "    # Проверка ответа: True - обоснован, False - содержит галлюцинации\n",
    "    if response.strip().lower() == \"true\":\n",
    "        print(\"The answer is grounded in the facts.\")\n",
    "        return \"grounded on context\"\n",
    "    else:\n",
    "        print(\"The answer is hallucination.\")\n",
    "        return \"hallucination\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_gpt_for_rating(row):\n",
    "    # Проверка на применимость требований\n",
    "    if pd.isna(row['Text_SSTS']):\n",
    "        return \"NA\"  # Возвращаем \"NA\", если требования не применимы\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Given comprehensive descriptions from multiple sources, analyze the text and classify it into one of the specific compliance categories based solely on the content provided. You should return ONLY the abbreviation of the compliance level without any additional text or explanation.\n",
    "\n",
    "    Categories:\n",
    "    - FC (Fully Compliant): The situation is perfect and nothing can be improved.\n",
    "    - LC (Largely Compliant): Generally correct, but some minor improvements may be needed. No full review is necessary.\n",
    "    - PC (Partially Compliant): There are major deviations. Significant improvements are needed and a subsequent review is required.\n",
    "    - NC (Non-Compliant): The requirements are not met, necessitating a complete redo and re-review.\n",
    "    - NA (Not Applicable): The situation described does not apply to the standards or requirements in question.\n",
    "\n",
    "    HMI Description: \"{row['Text_HMI']}\"\n",
    "    System Standards and Specifications (SSTS): \"{row['Text_SSTS']}\"\n",
    "    Key Discrepancies: \"{row['Requirement Compliance Answer']}\"\n",
    "    Differences: \"{row['Differences']}\"\n",
    "    Integrated Description: \"{row['Description']}\"\n",
    "\n",
    "    Based on the above information, please provide only the two-letter abbreviation (FC, LC, PC, NC, NA) that best describes the overall compliance level.\n",
    "    \"\"\"\n",
    "    messages = [Message(role=\"user\", content=prompt)]\n",
    "    payload = Payload(messages=messages)\n",
    "    response = sent_to_ai(payload)\n",
    "    return response.strip()  # Ensure only the abbreviation is returned with no extra spaces or characters\n",
    "\n",
    "# Применение функции к DataFrame\n",
    "df['Complience Level'] = df.apply(send_to_gpt_for_rating, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number                                        Name  \\\n",
      "0  30365  Start the Charging Process via Soft Switch   \n",
      "\n",
      "                                         Differences  \\\n",
      "0  **Key Compliance Issues:**\\n\\n1.  Lack of spec...   \n",
      "\n",
      "                                         Description Complience Level  \n",
      "0  **Charging Process via Soft Switch Summary**\\n...               NA  \n"
     ]
    }
   ],
   "source": [
    "submission = df[['Number', 'Name', 'Differences', 'Description', 'Complience Level']]\n",
    "submission.to_csv('submission-one.csv', index = False)\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения блокнота, разделенное на 15: 0.6638655503590901 секунд\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "result = total_time / 15\n",
    "print(f\"Время выполнения блокнота, разделенное на 15: {result} секунд\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
